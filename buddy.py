import nltk
import warnings
warnings.filterwarnings("ignore")
import numpy as np
import random
import string


f=open('nlp python answer finals.txt','r',errors = 'ignore')
m=open('modules pythons.txt','r',errors = 'ignore')
checkpoint = "./chatbot_weights.ckpt"
#session = tf.InteractiveSession()
#session.run(tf.global_variables_initializer())
#saver = tf.train.Saver()
#saver.restore(session, checkpoint)

raw=f.read()
rawone=m.read()
raw=raw.lower()# converts to lowercase
rawone=rawone.lower()# converts to lowercase
nltk.download('punkt') # first-time use only
nltk.download('wordnet') # first-time use only
sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences 
word_tokens = nltk.word_tokenize(raw)# converts to list of words
sent_tokensone = nltk.sent_tokenize(rawone)# converts to list of sentences 
word_tokensone = nltk.word_tokenize(rawone)# converts to list of words


sent_tokens[:2]
sent_tokensone[:2]

word_tokens[:5]
word_tokensone[:5]

lemmer = nltk.stem.WordNetLemmatizer()
def LemTokens(tokens):
    return [lemmer.lemmatize(token) for token in tokens]
remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)
def LemNormalize(text):
    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))

Introduce_Ans = ["My Name is Buddy.","I'm Buddy Your Daily Personal Assistant.","I am Buddy, Your Assistant","My name is Buddy and i am happy to help you"]
GREETING_INPUTS = ("hello Buddy","hello", "hi","hiii","hii","hiiii","hiiii", "greeting","buddy", "hey Buddy", "what's up","hey",)
GREETING_RESPONSES = ["Hello", "Hey", "Hii There","Hello, have a great day", "I am glad! You are talking to me", "Hello Dear, what can i do for you? "]
Basic_Q = ("who can developed you","who develop you","who creates you","your boss","your creator","your developer","your developers","who will develop you","how you created")
Basic_Ans = ("Rajan and its team will developed me", "I am created under the final year major project by Rajan and its team", "My Developers are :Mr Rajan, Mr Saurabh, Mr Vikas, Mrs Rubi and Mrs Anisha ","I'm created by Rajan and Its Team in August 2020")
Basic_Om = ("what can you do","your works","how can you help me","your task","your capablity","help me","your command","your commands","your features","your feature")
Basic_AnsM = ["My Features:-\nPlay music, Open programs, Search Wikipedia, Open Google, send E-mail, Play vedios on Youtube, social media, type and save text files, search medicines, shut down pc, set reminder, weather report, location and many "]


# Checking for greetings
def greeting(sentence):
    """If user's input is a greeting, return a greeting response"""
    for word in sentence.split():
        if word.lower() in GREETING_INPUTS:
            return random.choice(GREETING_RESPONSES)

# Checking for Basic_Q
def basic(sentence):
    for word in Basic_Q:
        if sentence.lower() == word:
            return random.choice(Basic_Ans)

# Checking for Basic_QM
def basicM(sentence):
    """If user's input is a greeting, return a greeting response"""
    for word in Basic_Om:
        if sentence.lower() == word:
            return random.choice(Basic_AnsM)
        
# Checking for Introduce
def IntroduceMe(sentence):
    return random.choice(Introduce_Ans)


from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# Generating response
def response(user_response):
    buddy_response=''
    sent_tokens.append(user_response)
    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')
    tfidf = TfidfVec.fit_transform(sent_tokens)
    vals = cosine_similarity(tfidf[-1], tfidf)
    idx=vals.argsort()[0][-2]
    flat = vals.flatten()
    flat.sort()
    req_tfidf = flat[-2]
    if(req_tfidf!=0):
        buddy_response = buddy_response+sent_tokens[idx]
        return buddy_response
    else:
        buddy_response=buddy_response+"I am sorry! I don't understand you"
        return buddy_response
      
# Generating response
def responseone(user_response):
    buddy_response=''
    sent_tokensone.append(user_response)
    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')
    tfidf = TfidfVec.fit_transform(sent_tokensone)
    vals = cosine_similarity(tfidf[-1], tfidf)
    idx=vals.argsort()[0][-2]
    flat = vals.flatten()
    flat.sort()
    req_tfidf = flat[-2]
    if(req_tfidf!=0):
        buddy_response = buddy_response+sent_tokens[idx]
        return buddy_response
    else:
        buddy_response=buddy_response+"I am sorry! I don't understand you"
        return buddy_response


def chat(user_response):
    user_response=user_response.lower()
    keyword = " module "
    keywordone = " module"
    keywordsecond = "module "
    
    if(user_response!='bye'):
        if(user_response=='thanks' or user_response=='thank you' ):
            flag=False
            #print("buddy: You are welcome..")
            return "You are welcome.."
        elif(basicM(user_response)!=None):
            return basicM(user_response)
        else:
            if(user_response.find(keyword) != -1 or user_response.find(keywordone) != -1 or user_response.find(keywordsecond) != -1):
                #print("buddy: ",end="")
                #print(responseone(user_response))
                return responseone(user_response)
                sent_tokensone.remove(user_response)
            elif(greeting(user_response)!=None):
                #print("buddy: "+greeting(user_response))
                return greeting(user_response)
            elif(user_response.find("your name") != -1 or user_response.find(" your name") != -1 or user_response.find("your name ") != -1 or user_response.find(" your name ") != -1):
                return IntroduceMe(user_response)
            elif(basic(user_response)!=None):
                return basic(user_response)
            else:
                #print("buddy: ",end="")
                #print(response(user_response))
                return response(user_response)
                sent_tokens.remove(user_response)
                
    else:
        flag=False
        return "Bye Bye! take care.."